\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{hyperref}

\title{Assignment 3 -- Moved Object Detection with DETR (Option 2: Pixel Differences)}
\author{Hailemariam Mersha (NetID: hbm9834)}
\date{\today}

\begin{document}

\maketitle

\section{Overview}
This report documents my implementation of Option~2 (pixel-difference input) to detect moved objects between two frames. I cover dataset handling, model/training choices, ablations over fine-tuning scope, results, and visuals. The pipeline is end-to-end, COCO-free, and uses the Hugging Face \texttt{DetrImageProcessor} to prepare inputs and targets safely.

\section{Method}
\paragraph{Data and split.} Frame pairs and matched annotation txt files are shuffled (fixed seed) and split 80/20 into train/val. Each match file has two lines per object (initial and final); I keep only the second-line (final/frame~2) boxes as ground truth. Tiny boxes are filtered; a dummy box is added only if needed to avoid empty batches.

\paragraph{Pre-processing.} I compute the pixel-wise absolute difference (RGB) between frame~1 and frame~2 as the model input. Images stay at native resolution; \texttt{DetrImageProcessor} handles resizing/normalization with explicit \texttt{shortest\_edge}/\texttt{longest\_edge} settings. The collate builds annotations on the fly, skips empty samples, and retains original paths/sizes for visualization.

\paragraph{Model.} I fine-tune \texttt{facebook/detr-resnet-50} with a 6-class head (unknown, person, car, other\_vehicle, other\_object, bike). The classifier/bbox heads are reinitialized; fine-tuning strategies include training all parameters, only the convolutional backbone, only the transformer block, or only the classification/bbox heads.

\paragraph{Training.} Optimizer: AdamW (lr $1{\times}10^{-5}$, weight decay $1{\times}10^{-4}$), batch size 2, 50 epochs. Gradients are clipped (max norm 1). Validation computes loss and precision/recall with score threshold 0.5 and IoU 0.5. Only the best checkpoint (lowest val loss) is saved.

\paragraph{Evaluation \& visualization.} \texttt{eval\_detr\_moved.py} computes precision/recall and generates four-panel visuals: top row shows initial and final GT (green) on frames~1 and~2; bottom row shows initial and final predictions (red with class labels/scores). Titles separate GT vs Pred panels. Ablations reuse the same eval to produce per-run metrics and visuals. (Insert sample visual here.)

\section{Results}
\subsection{Baseline training (50 epochs)}
\begin{itemize}[leftmargin=*]
  \item Best checkpoint: \texttt{outputs/checkpoints/detr\_option2\_all\_best.pth}
  \item Best val loss (from latest run): \textbf{1.1282}
  \item Best val precision/recall (score 0.5, IoU 0.5): \textbf{[fill after final eval]}
  \item Final epoch (50/50): val loss 1.1524; val precision/recall: 0.285 / 0.750
\end{itemize}
Final eval on the held-out split: precision = \textbf{[fill when run]}, recall = \textbf{[fill when run]} (same thresholds).

\subsection{Ablations (10 epochs each)}
I ran the required fine-tuning scopes: all parameters, convolutional backbone only, transformer block only, and classification/bbox heads only (plus a higher LR for ``all''). Current results below are from the latest run (50 epochs per ablation); config is now set to 10 epochs for faster reruns.
\begin{center}
\begin{tabular}{@{}lcccc@{}}
\toprule
Setting & LR & Val Loss & Precision & Recall \\
\midrule
all\_lr1e-5 & $1{\times}10^{-5}$ & 1.1735 & 0.254 & 0.783 \\
all\_lr5e-5 & $5{\times}10^{-5}$ & 2.9377 & 0.000 & 0.000 \\
backbone\_lr1e-5 & $1{\times}10^{-5}$ & 2.0532 & 0.000 & 0.000 \\
transformer\_lr1e-5 & $1{\times}10^{-5}$ & [not run yet] & [fill] & [fill] \\
head\_lr1e-5 & $1{\times}10^{-5}$ & 1.4292 & 0.750 & 0.025 \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Qualitative}
Four-panel visuals are written to:
\begin{itemize}[leftmargin=*]
  \item Baseline eval: \texttt{outputs/eval\_vis/}
  \item Ablations: \texttt{outputs/ablation\_eval\_vis/<run\_name>/}
\end{itemize}
Each image: top row GT (initial/final) in green; bottom row predictions (initial/final) in red with class names and scores. (Insert representative images here.)

\section{Design decisions and rationale}
\begin{itemize}[leftmargin=*]
  \item Pixel diff input (Option~2) highlights moved content directly, avoiding extra motion cues.
  \item Processor-based pipeline reduces manual preprocessing and empty-target issues; explicit sizing avoids deprecated params.
  \item COCO-free: targets are built from match files on the fly, simplifying data flow.
  \item Best-checkpoint only: minimizes storage and keeps eval simple.
  \item Thresholds: 0.5/0.5 to limit false positives in visuals; can be relaxed if recall is too low.
  \item Ablations: required fine-tuning scopes to compare what to unfreeze; more epochs likely improve convergence if time permits.
\end{itemize}

\section{Reproducibility}
Run \texttt{sbatch slurm/task3\_job.slurm} (update paths to scratch). Outputs:
\begin{itemize}[leftmargin=*]
  \item Metrics: \texttt{outputs/ablation\_metrics/}, \texttt{outputs/eval/metrics.txt}
  \item Visuals: \texttt{outputs/ablation\_eval\_vis/}, \texttt{outputs/eval\_vis/}
\end{itemize}

\section{Pending items}
\begin{itemize}[leftmargin=*]
  \item Fill in final eval precision/recall for the baseline checkpoint.
  \item Fill in remaining ablation metrics (transformer\_lr1e-5 and any 10-epoch reruns if performed).
  \item Insert representative visualizations in the qualitative section.
\end{itemize}

\end{document}
